---
layout: post
title: Image Effects | Part 7 - Wrapping Up
subtitle: Cleaning up a few unexplored concepts
bigimg: /img/tut1/part7-banner.png
gh-repo: daniel-ilett/smo-shaders
gh-badge: [star, fork, follow]
tags: [shaders, unity, image-effects, outro]
nice-slug: Wrapping Up
date: 2019-05-22
---

We've written a lot of shaders so far! Hopefully you have a solid grasp on the basics of image effect shaders and have some ideas on where to take things in your own time. This article will recap what we've learned, and go into a bit more detail on some aspects of shader programming that were glossed over a little in previous articles. I hope this isn't too much of an info-dump!

## ShaderLab, Nvidia Cg, GLSL and HLSL

`HLSL` stands for High-Level Shading Language, and `GLSL` stands for Open GL Shading Language. When you write code in a shading language, it is compiled into a special kind of program that can run directly on a GPU (Graphics Processing Unit). Unity provides its own special language called `ShaderLab`, which acts as a middleman between the Unity Engine and the shading language, providing several shortcuts and macros to make life easier for us. The actual shading language you write inside a ShaderLab file is, by default, `Nvidia Cg`, a portable language which uses the common feature set of GLSL and HLSL for compatability with a wide range of devices. Its syntax is close to that of HLSL.

## Object Space, World Space, Clip Space and Screen Space

A 3D model, saved in a format such as `.obj` or `.fbx`, stores a list of every vertex in the image, as well as their positions. That position data is relative to some 'origin point' of the image, and usually doesn't have any inherent meaning - a vertex with position (2, 0, 0) is just twice as far away from the origin as a vertex with position (1, 0, 0). This is called `object space`.

Inside a scene in Unity, you could have several different models. Each vertex of each model is no longer positioned relative to its own model; it is positioned relative to some new 'world' origin point. Or, more accurately, a vertex is positioned relative to its own object origin point, and the object origin point is positioned relative to a world origin point. This is called `world space`.

So far, none of these spaces are taking a 'viewer' such as a camera or a person into account. When you place a camera into a scene in Unity, it has several properties of its own:
* Near clip distance: any object closer to the camera than the near clip distance is not rendered (it is `culled`);
* Far clip distance: any object further away from the camera than the far clip distance is also culled;
* Field-of-view (FOV): Assuming the near and far clip distances are used to define the top and bottom of a rectangular-based pyramid with the top cut off, the FOV describes the 'angle' or 'steepness' of the pyramid.

The pyramid shape described here is the `view frustum`. Objects outside of it are culled. The objects that are left have their positions transformed into a new space called `clip space`, and a process called `projection` transforms the positions such that the x-, y- and z-coordinates are normalised between 0 and 1. Positions touching the left edge or right edge have x-positions of 0 or 1 respectively; positions on the bottom edge or top edge of the frustum have y-positions or 0 or 1 respectively; and positions touching the near clip plane or far clip plane have z-positions of 0 or 1 respectively. Sometimes the y-axis or z-axis is flipped, depending on the platform or the graphics API being used.

## Structs

A `struct` is exactly the same in a shading language as it is in `C`; it is a collection of data. More accurately, it is a datatype that we can use in place of other datatype such as `float` or `int`, and we can use a struct type to return several variables from a function.

~~~glsl
struct myStruct
{
    float3 myVector;
    int myInt;
};
~~~

## UV Coordinates

## Texture Sampling

## Vertex and Fragment Shaders

A `vertex shader` takes in a list of vertices of a model, as well as properties of those vertices: their position in object space, their UV coordinates, a vertex colour, and so on. The vertex shader transforms the input data into a format usable by the fragment shader.

Data is passed to the vertex shader inside a struct - this is commonly named `appdata`, as we saw. This struct contains the vertex properties I just listed.

Between the vertex shader and the fragment shader stages of the graphics pipeline, a process called `rasterisation` fills the space between the vertices and convertx the resulting region into a collection of `fragments` - usually, a fragment corresponds to a pixel on the screen. Hence, a fragment shader is sometimes known as a 'pixel shader'.

The `fragment shader` receives data output by the vertex shader inside a struct. That struct is commonly called `v2f`, and it can contain more or fewer variables than `appdata`, or even completely unrelated data. The fragment shader operates on every fragment/pixel sent to it, and outputs a colour value to be output to the screen.

## The Depth Buffer

## Graphics.Blit

## Wrap Mode

## RGB and HSV Colour Space

## Transparency

## Swizzling

## UsePass

## Matrix Multiplication

## Supersampling and Subsampling

## Lerp, Saturate and Step Functions

<hr/>

# Conclusion

Thus concludes the first part of the Super Mario Odyssey shaders series. I have more effects planned for future posts, but I'd like to move on to something else for a little while to keep things exciting.

<hr/>
